{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicoding Belajar Machine Learning Untuk Pemula Final Project : Image Classification\n",
    "\n",
    "- **Nama**         : Azarya Yehezkiel Pinondang Sipahutar\n",
    "- **Email**         : azaryaemc@gmail.com\n",
    "- **ID Dicoding**   : azarya_yehezkiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries for file manipulation and visualization\n",
    "import zipfile, os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Import the necessary libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the necessary libraries for deep learning with TensorFlow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Print the version of TensorFlow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, we load the images from our dataset and perform some preprocessing such as rescaling the images. We also split our dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the zip file containing the images is defined\n",
    "local_zip = 'rockpaperscissors.zip'\n",
    "\n",
    "# The zip file is opened in read mode\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "\n",
    "# The contents of the zip file are extracted to the '/tmp' directory\n",
    "zip_ref.extractall('/tmp')\n",
    "\n",
    "# The zip file is closed\n",
    "zip_ref.close()\n",
    "\n",
    "# The base directory where the images are located is defined\n",
    "base_dir = '/tmp/rockpaperscissors/rps-cv-images'\n",
    "\n",
    "# The directories for the 'rock', 'paper', and 'scissors' images are defined\n",
    "rock_dir = os.path.join(base_dir, 'rock')\n",
    "paper_dir = os.path.join(base_dir, 'paper')\n",
    "scissors_dir = os.path.join(base_dir, 'scissors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train(60%) Test (40%) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories for the training and validation datasets\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "# Create these directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# We're using a 60/40 split, so 60% of the images will be used for training, and 40% for validation\n",
    "# The train_test_split function shuffles the images before splitting them, to ensure a good mix of images in both sets\n",
    "train_rock_dir, val_rock_dir = train_test_split(os.listdir(rock_dir), test_size = 0.4)\n",
    "train_paper_dir, val_paper_dir = train_test_split(os.listdir(paper_dir), test_size = 0.4)\n",
    "train_scissors_dir, val_scissors_dir = train_test_split(os.listdir(scissors_dir), test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to move files\n",
    "def move_files(files, src_dir, dst_dir):\n",
    "    \"\"\"\n",
    "    Move a list of files from a source directory to a destination directory.\n",
    "\n",
    "    Parameters:\n",
    "    files (list): A list of filenames to be moved.\n",
    "    src_dir (str): The directory where the files currently reside.\n",
    "    dst_dir (str): The directory where the files should be moved to.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    for file in files:\n",
    "        shutil.move(os.path.join(src_dir, file), os.path.join(dst_dir, file))\n",
    "\n",
    "# Move the files for 'rock' category\n",
    "move_files(train_rock_dir, rock_dir, os.path.join(train_dir, 'rock'))\n",
    "move_files(val_rock_dir, rock_dir, os.path.join(validation_dir, 'rock'))\n",
    "\n",
    "# Move the files for 'paper' category\n",
    "move_files(train_paper_dir, paper_dir, os.path.join(train_dir, 'paper'))\n",
    "move_files(val_paper_dir, paper_dir, os.path.join(validation_dir, 'paper'))\n",
    "\n",
    "# Move the files for 'scissors' category\n",
    "move_files(train_scissors_dir, scissors_dir, os.path.join(train_dir, 'scissors'))\n",
    "move_files(val_scissors_dir, scissors_dir, os.path.join(validation_dir, 'scissors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper', 'rock', 'scissors']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show train directory\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper', 'rock', 'scissors']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show validation directory\n",
    "os.listdir(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2181 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0,1]\n",
    "    rotation_range=20,  \n",
    "    horizontal_flip=True,  \n",
    "    shear_range = 0.2,  \n",
    "    fill_mode = 'nearest',  \n",
    ")\n",
    "\n",
    "# Load images from the disk, applies data augmentation, and resizes the images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150, 150),\n",
    "    batch_size=5,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2089 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ImageDataGenerator for validation data\n",
    "test_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255 # Normalize pixel values  \n",
    ")\n",
    "\n",
    "# Load images from the disk, rescale them, and resize tahe images\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,  \n",
    "    target_size=(150, 150),\n",
    "    batch_size=5,\n",
    "    class_mode='categorical'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mappings for classes present in the training and validation datasets\n",
      "\n",
      "0 : paper\n",
      "1 : rock\n",
      "2 : scissors\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map the class indices to their respective labels\n",
    "labels = {value: key for key, value in train_generator.class_indices.items()}\n",
    "\n",
    "print(\"Label Mappings for classes present in the training and validation datasets\\n\")\n",
    "# Print each class index and its corresponding label\n",
    "for key, value in labels.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Here we build our image classification model. We're using a Convolutional Neural Network (CNN) which is commonly used in image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\azary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\azary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    # First convolution layer, 32 filters of size 3x3, activation function is ReLU\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    # First max pooling layer with pool size 2x2\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    # Second convolution layer, 64 filters of size 3x3, activation function is ReLU\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    # Second max pooling layer with pool size 2x2\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    # Third convolution layer, 128 filters of size 3x3, activation function is ReLU\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    # Third max pooling layer with pool size 2x2\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    # Fourth convolution layer, 256 filters of size 3x3, activation function is ReLU\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    # Fourth max pooling layer with pool size 2x2\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    # Flatten layer to convert the 3D feature maps to 1D feature vectors\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Fully connected layer with 512 neurons, activation function is ReLU\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Fully connected layer with 256 neurons, activation function is ReLU\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "    # Output layer with 3 neurons (for 3 classes), activation function is softmax\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 7, 7, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               6423040   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6943555 (26.49 MB)\n",
      "Trainable params: 6943555 (26.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    # Use RMSprop optimizer\n",
    "    optimizer=tf.optimizers.RMSprop(),\n",
    "    # Use Kullback-Leibler divergence as the loss function\n",
    "    loss='kullback_leibler_divergence',\n",
    "    # Track accuracy as a metric during training\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now that our model is built, we can train it using our training data. We also validate our model using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\azary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\azary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "437/437 - 29s - loss: 0.6571 - accuracy: 0.7093 - val_loss: 0.1817 - val_accuracy: 0.9449 - 29s/epoch - 65ms/step\n",
      "Epoch 2/10\n",
      "437/437 - 28s - loss: 0.2715 - accuracy: 0.9156 - val_loss: 0.1176 - val_accuracy: 0.9674 - 28s/epoch - 64ms/step\n",
      "Epoch 3/10\n",
      "437/437 - 31s - loss: 0.1877 - accuracy: 0.9390 - val_loss: 0.0891 - val_accuracy: 0.9732 - 31s/epoch - 71ms/step\n",
      "Epoch 4/10\n",
      "437/437 - 29s - loss: 0.1509 - accuracy: 0.9509 - val_loss: 0.2253 - val_accuracy: 0.9167 - 29s/epoch - 66ms/step\n",
      "Epoch 5/10\n",
      "437/437 - 30s - loss: 0.1279 - accuracy: 0.9647 - val_loss: 0.1006 - val_accuracy: 0.9775 - 30s/epoch - 69ms/step\n",
      "Epoch 6/10\n",
      "437/437 - 29s - loss: 0.1628 - accuracy: 0.9619 - val_loss: 0.0556 - val_accuracy: 0.9885 - 29s/epoch - 67ms/step\n",
      "Epoch 7/10\n",
      "437/437 - 29s - loss: 0.1090 - accuracy: 0.9702 - val_loss: 0.0410 - val_accuracy: 0.9904 - 29s/epoch - 66ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Reached 98% accuracy so cancelling training!\n",
      "437/437 - 30s - loss: 0.0790 - accuracy: 0.9826 - val_loss: 0.0279 - val_accuracy: 0.9947 - 30s/epoch - 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x229ed5f7c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CallbackEpoch(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    This class inherits from the `tf.keras.callbacks.Callback` class and overrides the `on_epoch_end` method.\n",
    "    It's used to stop the training process once a certain accuracy threshold is reached.\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # If accuracy > 98%, stop training\n",
    "        if logs.get('accuracy') > 0.98 and logs.get('val_accuracy') > 0.98:\n",
    "            print(\"\\nReached 98% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Instantiate the custom callback\n",
    "callback_epoch = CallbackEpoch()\n",
    "\n",
    "# Train Model \n",
    "model.fit(train_generator, \n",
    "          epochs=10, \n",
    "          validation_data=validation_generator, \n",
    "          verbose=2, \n",
    "          callbacks=[callback_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation to new pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf18721344b46ec87dc3b89dd540be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a FileUpload widget\n",
    "uploader = widgets.FileUpload(multiple=False)\n",
    "display(uploader)\n",
    "\n",
    "def on_upload_change(change):\n",
    "    # Get the uploaded file\n",
    "    uploaded = change['new']\n",
    "    for fn, file_info in uploaded.items():\n",
    "        with io.BytesIO(file_info['content']) as f:\n",
    "            img = Image.open(f).resize((150, 150))\n",
    "        imgplot = plt.imshow(img)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        images = np.vstack([x])\n",
    "\n",
    "        print(\"Image preprocessed\")  # Debug print\n",
    "\n",
    "        # Make a prediction\n",
    "        classes = model.predict(images, batch_size=10)\n",
    "        print(\"Prediction made:\", classes)  # Debug print\n",
    "\n",
    "        print(fn)\n",
    "        if classes[0,0] != 0:\n",
    "            print('paper')\n",
    "        elif classes[0,1] != 0:\n",
    "            print('rock')\n",
    "        else:\n",
    "            print('scissors')\n",
    "\n",
    "# Attach the function to the uploader\n",
    "uploader.observe(on_upload_change, names='_counter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
