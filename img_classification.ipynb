{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicoding Belajar Machine Learning Untuk Pemula Final Project : Image Classification\n",
    "\n",
    "- **Nama**         : Azarya Yehezkiel Pinondang Sipahutar\n",
    "- **Email**         : azaryaemc@gmail.com\n",
    "- **ID Dicoding**   : azarya_yehezkiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\azary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import zipfile,os\n",
    "import shutil\n",
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, we load the images from our dataset and perform some preprocessing such as rescaling the images. We also split our dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path of the zip file\n",
    "local_zip = 'rockpaperscissors.zip'\n",
    "\n",
    "# Open the zip file in read mode\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "\n",
    "# Extract all the contents of the zip file in /tmp directory\n",
    "zip_ref.extractall('/tmp')\n",
    "\n",
    "# Close the ZipFile object\n",
    "# zip_ref.close()\n",
    "\n",
    "# Define the base directory where the images are located\n",
    "base_dir = '/tmp/rockpaperscissors/rps-cv-images'\n",
    "\n",
    "# Define the directory for the training data\n",
    "rock_dir = os.path.join(base_dir, 'rock')\n",
    "paper_dir = os.path.join(base_dir, 'paper')\n",
    "scissors_dir = os.path.join(base_dir, 'scissors') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train(60%) Test (40%) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Split the data by train/test for each directory\n",
    "train_rock_dir, val_rock_dir = train_test_split(os.listdir(rock_dir), test_size = 0.4)\n",
    "train_paper_dir, val_paper_dir = train_test_split(os.listdir(paper_dir), test_size = 0.4)\n",
    "train_scissors_dir, val_scissors_dir = train_test_split(os.listdir(scissors_dir), test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to move files\n",
    "def move_files(files, src_dir, dst_dir):\n",
    "    os.makedirs(dst_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    for file in files:\n",
    "        shutil.move(os.path.join(src_dir, file), os.path.join(dst_dir, file))\n",
    "\n",
    "# Move the files\n",
    "move_files(train_rock_dir, rock_dir, os.path.join(train_dir, 'rock'))\n",
    "move_files(val_rock_dir, rock_dir, os.path.join(validation_dir, 'rock'))\n",
    "\n",
    "move_files(train_paper_dir, paper_dir, os.path.join(train_dir, 'paper'))\n",
    "move_files(val_paper_dir, paper_dir, os.path.join(validation_dir, 'paper'))\n",
    "\n",
    "move_files(train_scissors_dir, scissors_dir, os.path.join(train_dir, 'scissors'))\n",
    "move_files(val_scissors_dir, scissors_dir, os.path.join(validation_dir, 'scissors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1814 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=20,\n",
    "                    horizontal_flip=True,\n",
    "                    shear_range = 0.2,\n",
    "                    fill_mode = 'nearest',\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # should contain 3 subdirectories, one for each class\n",
    "    target_size=(200, 200),\n",
    "    batch_size=25,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1378 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255\n",
    ")\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,  # should contain 3 subdirectories, one for each class\n",
    "    target_size=(200, 200),\n",
    "    batch_size=25,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mappings for classes present in the training and validation datasets\n",
      "\n",
      "0 : paper\n",
      "1 : rock\n",
      "2 : scissors\n"
     ]
    }
   ],
   "source": [
    "labels = {value: key for key, value in train_generator.class_indices.items()}\n",
    "\n",
    "print(\"Label Mappings for classes present in the training and validation datasets\\n\")\n",
    "for key, value in labels.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'paper', 1: 'rock', 2: 'scissors'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Here we build our image classification model. We're using a Convolutional Neural Network (CNN) which is commonly used in image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(200,200,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 74, 74, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 36, 36, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 17, 17, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 15, 15, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPooli  (None, 7, 7, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               6423040   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6976067 (26.61 MB)\n",
      "Trainable params: 6976067 (26.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.RMSprop(),\n",
    "              loss='kullback_leibler_divergence',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now that our model is built, we can train it using our training data. We also validate our model using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "91/91 - 18s - loss: 0.0560 - accuracy: 0.9851 - val_loss: 0.0135 - val_accuracy: 0.9971 - 18s/epoch - 201ms/step\n",
      "Epoch 2/15\n",
      "91/91 - 17s - loss: 0.0410 - accuracy: 0.9890 - val_loss: 0.0105 - val_accuracy: 0.9971 - 17s/epoch - 189ms/step\n",
      "Epoch 3/15\n",
      "91/91 - 17s - loss: 0.0404 - accuracy: 0.9923 - val_loss: 0.0073 - val_accuracy: 0.9978 - 17s/epoch - 185ms/step\n",
      "Epoch 4/15\n",
      "91/91 - 16s - loss: 0.0337 - accuracy: 0.9884 - val_loss: 0.0051 - val_accuracy: 0.9985 - 16s/epoch - 180ms/step\n",
      "Epoch 5/15\n",
      "91/91 - 16s - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.0183 - val_accuracy: 0.9949 - 16s/epoch - 180ms/step\n",
      "Epoch 6/15\n",
      "91/91 - 16s - loss: 0.0352 - accuracy: 0.9906 - val_loss: 0.0035 - val_accuracy: 0.9978 - 16s/epoch - 178ms/step\n",
      "Epoch 7/15\n",
      "91/91 - 16s - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.2106 - val_accuracy: 0.9673 - 16s/epoch - 180ms/step\n",
      "Epoch 8/15\n",
      "91/91 - 16s - loss: 0.0224 - accuracy: 0.9912 - val_loss: 0.0037 - val_accuracy: 0.9985 - 16s/epoch - 179ms/step\n",
      "Epoch 9/15\n",
      "91/91 - 16s - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.0079 - val_accuracy: 0.9971 - 16s/epoch - 180ms/step\n",
      "Epoch 10/15\n",
      "91/91 - 16s - loss: 0.0320 - accuracy: 0.9945 - val_loss: 0.0023 - val_accuracy: 0.9985 - 16s/epoch - 181ms/step\n",
      "Epoch 11/15\n",
      "91/91 - 16s - loss: 0.0143 - accuracy: 0.9945 - val_loss: 0.0029 - val_accuracy: 0.9985 - 16s/epoch - 180ms/step\n",
      "Epoch 12/15\n",
      "91/91 - 19s - loss: 0.0249 - accuracy: 0.9939 - val_loss: 0.0053 - val_accuracy: 0.9985 - 19s/epoch - 204ms/step\n",
      "Epoch 13/15\n",
      "91/91 - 19s - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.0372 - val_accuracy: 0.9898 - 19s/epoch - 210ms/step\n",
      "Epoch 14/15\n",
      "91/91 - 19s - loss: 0.0277 - accuracy: 0.9928 - val_loss: 0.0114 - val_accuracy: 0.9964 - 19s/epoch - 213ms/step\n",
      "Epoch 15/15\n",
      "91/91 - 19s - loss: 0.0322 - accuracy: 0.9928 - val_loss: 0.0014 - val_accuracy: 1.0000 - 19s/epoch - 210ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a72d4afd50>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
